{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cs3244blstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9nRZPP3ZR2K5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "103507f4-8400-4f52-8716-f3b5bc55379f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import csv\n",
        "from google.colab import drive\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxLength = 0\n",
        "text = []\n",
        "classification = []\n",
        "temp = []\n",
        "with open('/content/drive/My Drive/cs3244/fulltrain.csv','r') as f:\n",
        "  data = csv.reader(f)\n",
        "  for row in data:\n",
        "    temp.append((row[0],row[1]))\n",
        "random.shuffle(temp)\n",
        "for i in range(500):\n",
        "  text.append(temp[i][1])\n",
        "  if len(temp[i][1].split()) > maxLength:\n",
        "    maxLength = len(temp[i][1].split())\n",
        "  classification.append(int(temp[i][0])-1)\n",
        "print(classification)\n",
        "print(maxLength)"
      ],
      "metadata": {
        "id": "Q0TKbVVDT5wJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10b4ee20-bf0d-44d6-d5e0-64d936ddd996"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 0, 0, 0, 2, 0, 2, 2, 2, 1, 2, 2, 1, 2, 3, 0, 0, 0, 2, 3, 2, 3, 0, 2, 2, 1, 3, 3, 1, 2, 1, 3, 0, 0, 0, 0, 3, 0, 0, 1, 2, 2, 0, 0, 2, 2, 2, 3, 1, 1, 1, 2, 3, 3, 2, 1, 3, 1, 1, 2, 3, 1, 2, 2, 0, 3, 2, 1, 2, 3, 2, 2, 0, 3, 2, 0, 3, 1, 3, 2, 1, 2, 2, 2, 2, 2, 1, 3, 0, 0, 2, 0, 2, 2, 2, 2, 0, 0, 0, 1, 0, 3, 1, 1, 3, 0, 2, 2, 2, 3, 2, 0, 0, 1, 3, 2, 1, 0, 1, 0, 2, 2, 2, 2, 1, 0, 2, 1, 0, 1, 1, 3, 0, 2, 2, 3, 1, 2, 3, 0, 0, 2, 0, 0, 2, 0, 1, 0, 3, 2, 2, 1, 3, 2, 0, 3, 2, 2, 2, 2, 0, 0, 1, 3, 0, 1, 3, 0, 2, 2, 2, 2, 0, 3, 2, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 1, 0, 1, 2, 2, 3, 2, 3, 1, 2, 2, 3, 2, 3, 0, 2, 3, 1, 3, 3, 2, 1, 3, 1, 3, 2, 1, 0, 0, 3, 2, 2, 1, 2, 1, 2, 0, 0, 2, 2, 2, 1, 0, 3, 2, 2, 3, 0, 2, 3, 3, 2, 2, 1, 2, 2, 3, 2, 0, 2, 2, 2, 1, 0, 2, 2, 2, 3, 0, 1, 2, 2, 1, 2, 0, 2, 1, 1, 2, 1, 2, 0, 2, 2, 2, 0, 2, 0, 0, 1, 1, 2, 3, 0, 1, 2, 0, 2, 3, 0, 2, 3, 1, 2, 0, 2, 2, 0, 3, 0, 2, 0, 1, 2, 3, 3, 0, 0, 2, 2, 0, 2, 3, 2, 0, 1, 3, 0, 3, 0, 3, 0, 0, 2, 0, 0, 3, 1, 2, 1, 3, 0, 2, 2, 1, 2, 0, 0, 2, 1, 2, 1, 2, 3, 3, 3, 0, 1, 0, 0, 2, 2, 0, 0, 2, 0, 2, 2, 2, 3, 3, 3, 0, 0, 2, 0, 3, 3, 0, 3, 2, 2, 0, 3, 2, 3, 2, 2, 3, 2, 0, 2, 0, 2, 2, 2, 2, 1, 3, 1, 1, 3, 2, 2, 0, 2, 2, 3, 0, 2, 0, 0, 3, 3, 3, 0, 2, 2, 0, 3, 2, 2, 0, 2, 0, 2, 1, 0, 3, 1, 0, 0, 3, 2, 2, 2, 0, 0, 2, 0, 3, 3, 2, 3, 0, 0, 3, 0, 2, 0, 2, 2, 0, 2, 0, 0, 1, 2, 2, 2, 2, 3, 3, 0, 3, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 2, 2, 1, 2, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 3, 0, 2, 2, 3, 3, 0, 2, 1, 2, 2, 0, 2, 0, 1, 3, 2, 0, 1, 2, 2, 0, 3, 2, 2, 2]\n",
            "8384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text)\n",
        "word_index = tokenizer.word_index\n",
        "X = tokenizer.texts_to_sequences(text)\n",
        "X = pad_sequences(X, padding = \"post\", truncating = \"post\", maxlen = maxLength)\n",
        "y = [to_categorical(i,num_classes = 4) for i in classification]\n",
        "#y = classification"
      ],
      "metadata": {
        "id": "liA33L5_O3ai"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed = {}\n",
        "f = open('drive/MyDrive/glove.6B.100d.txt','r')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = np.asarray(values[1:], dtype = \"float32\")\n",
        "\tembed [word] = coefs\n",
        "f.close()"
      ],
      "metadata": {
        "id": "akidn1Gy8Euv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embed.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "6Xz-XfIcPAUy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(np.array(y).shape)\n",
        "\n",
        "keywordModel = Sequential()\n",
        "keywordModel.add(Embedding(len(word_index) + 1, 100,input_length=maxLength, weights = [embedding_matrix]))\n",
        "keywordModel.add(Bidirectional(LSTM(100)))\n",
        "keywordModel.add(Dense(100, activation = \"relu\"))\n",
        "keywordModel.add(Dense(4, activation = \"softmax\"))\n",
        "keywordModel.compile(loss=\"categorical_crossentropy\", optimizer = \"adam\")\n",
        "keywordModel.fit(X, np.array(y), batch_size = 32, epochs = 10, validation_split = 0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NROqJNVZ830x",
        "outputId": "687c4e76-3ab7-4c3f-8a9b-83b6a31fad47"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 8384)\n",
            "(500, 4)\n",
            "Epoch 1/10\n",
            "15/15 [==============================] - 310s 20s/step - loss: 1.3082 - val_loss: 1.2402\n",
            "Epoch 2/10\n",
            "15/15 [==============================] - 304s 20s/step - loss: 1.0670 - val_loss: 1.1149\n",
            "Epoch 3/10\n",
            "15/15 [==============================] - 299s 20s/step - loss: 0.8385 - val_loss: 0.8821\n",
            "Epoch 4/10\n",
            "15/15 [==============================] - 303s 20s/step - loss: 0.6595 - val_loss: 0.9729\n",
            "Epoch 5/10\n",
            "15/15 [==============================] - 307s 21s/step - loss: 0.5476 - val_loss: 0.8046\n",
            "Epoch 6/10\n",
            "15/15 [==============================] - 305s 20s/step - loss: 0.3735 - val_loss: 0.6259\n",
            "Epoch 7/10\n",
            "15/15 [==============================] - 301s 20s/step - loss: 0.2144 - val_loss: 0.7196\n",
            "Epoch 8/10\n",
            "15/15 [==============================] - 306s 20s/step - loss: 0.1343 - val_loss: 0.8344\n",
            "Epoch 9/10\n",
            "15/15 [==============================] - 298s 20s/step - loss: 0.0881 - val_loss: 0.6786\n",
            "Epoch 10/10\n",
            "15/15 [==============================] - 297s 20s/step - loss: 0.0584 - val_loss: 0.8180\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f157bd84690>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textTest = []\n",
        "classificationTest = []\n",
        "with open('/content/drive/My Drive/cs3244/balancedtest.csv','r') as f:\n",
        "  data = csv.reader(f)\n",
        "  for row in data:\n",
        "    temp.append((row[0],row[1]))\n",
        "random.shuffle(temp)\n",
        "for i in range(500):\n",
        "  textTest.append(temp[i][1])\n",
        "  classificationTest.append(int(temp[i][0])-1)\n",
        "XTest = tokenizer.texts_to_sequences(textTest)\n",
        "XTest = pad_sequences(XTest, padding = \"post\", truncating = \"post\", maxlen = maxLength)\n",
        "yTest = [to_categorical(i,num_classes = 4) for i in classificationTest]"
      ],
      "metadata": {
        "id": "w13oaXPPc0AD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(XTest.shape)\n",
        "a = keywordModel.predict(XTest)\n",
        "a = np.argmax(a, axis = -1)\n",
        "flattened_actual = (np.argmax(np.array(yTest), axis = -1)).flatten()\n",
        "flattened_output = a.flatten()\n",
        "print(classification_report(flattened_actual, flattened_output))\n",
        "print(f1_score(y_pred=flattened_actual, y_true=flattened_output,average = \"macro\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WupRu2GxllN1",
        "outputId": "5fbd78a5-ed10-4a53-eb1e-b0a7381987b6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 8384)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.77      0.78       135\n",
            "           1       0.65      0.80      0.72        89\n",
            "           2       0.82      0.74      0.78       171\n",
            "           3       0.72      0.72      0.72       105\n",
            "\n",
            "    accuracy                           0.75       500\n",
            "   macro avg       0.75      0.76      0.75       500\n",
            "weighted avg       0.76      0.75      0.76       500\n",
            "\n",
            "0.7488480183985803\n"
          ]
        }
      ]
    }
  ]
}