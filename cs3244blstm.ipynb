{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cs3244blstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9nRZPP3ZR2K5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8d3e5b2-c190-4aaf-dd82-ffb1de02dbc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import csv\n",
        "from google.colab import drive\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxLength = 0\n",
        "text = []\n",
        "classification = []\n",
        "temp = []\n",
        "with open('/content/drive/My Drive/cs3244/fulltrain.csv','r') as f:\n",
        "  data = csv.reader(f)\n",
        "  for row in data:\n",
        "    temp.append((row[0],row[1]))\n",
        "random.shuffle(temp)\n",
        "for i in range(500):\n",
        "  text.append(temp[i][1])\n",
        "  if len(temp[i][1].split()) > maxLength:\n",
        "    maxLength = len(temp[i][1].split())\n",
        "  classification.append(int(temp[i][0])-1)\n",
        "print(classification)\n",
        "print(maxLength)"
      ],
      "metadata": {
        "id": "Q0TKbVVDT5wJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4003d912-0c59-4ca2-f1db-4f708b9c0296"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 3, 0, 0, 2, 2, 2, 3, 3, 0, 0, 2, 2, 0, 3, 2, 2, 3, 0, 0, 1, 2, 0, 1, 0, 0, 3, 0, 2, 0, 2, 0, 0, 2, 0, 1, 1, 0, 3, 2, 1, 0, 0, 0, 3, 3, 2, 0, 2, 2, 0, 0, 2, 2, 2, 1, 0, 0, 2, 2, 2, 3, 0, 1, 0, 0, 2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 0, 1, 2, 0, 1, 1, 0, 2, 1, 2, 3, 1, 2, 2, 2, 0, 0, 1, 2, 0, 0, 1, 1, 0, 3, 2, 0, 0, 1, 3, 0, 1, 2, 1, 0, 0, 3, 1, 2, 0, 3, 1, 2, 3, 1, 0, 0, 1, 2, 3, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 3, 0, 3, 1, 3, 2, 0, 2, 0, 0, 0, 1, 1, 2, 0, 2, 0, 2, 3, 0, 1, 0, 2, 2, 2, 3, 3, 3, 1, 0, 2, 2, 0, 1, 2, 2, 2, 2, 1, 0, 2, 2, 2, 2, 2, 0, 0, 0, 3, 3, 0, 2, 0, 3, 2, 1, 0, 2, 0, 3, 2, 2, 0, 1, 3, 2, 2, 2, 3, 2, 0, 3, 2, 0, 2, 0, 3, 2, 3, 2, 1, 3, 2, 2, 1, 0, 3, 0, 2, 0, 3, 2, 0, 3, 3, 1, 1, 2, 3, 2, 3, 0, 0, 2, 2, 2, 1, 0, 2, 0, 0, 2, 0, 0, 2, 0, 0, 1, 3, 2, 2, 2, 3, 2, 1, 3, 2, 2, 2, 2, 0, 3, 2, 3, 0, 2, 2, 2, 2, 2, 0, 0, 2, 3, 1, 1, 2, 3, 1, 3, 2, 2, 0, 0, 2, 2, 0, 2, 3, 0, 0, 2, 2, 2, 0, 3, 3, 0, 1, 2, 0, 3, 0, 2, 1, 2, 2, 1, 3, 2, 2, 2, 3, 2, 1, 2, 2, 0, 0, 0, 2, 2, 1, 0, 0, 3, 2, 0, 3, 3, 2, 0, 1, 2, 0, 0, 1, 1, 2, 0, 3, 3, 1, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 0, 1, 3, 1, 2, 0, 2, 0, 2, 1, 0, 2, 3, 2, 1, 0, 1, 0, 3, 2, 3, 0, 0, 3, 0, 2, 2, 2, 2, 3, 3, 2, 3, 1, 2, 2, 1, 3, 1, 0, 1, 0, 3, 3, 2, 0, 2, 2, 1, 0, 3, 2, 3, 2, 3, 1, 0, 2, 3, 2, 3, 3, 2, 2, 0, 1, 1, 3, 2, 0, 2, 0, 2, 2, 3, 3, 2, 3, 0, 1, 1, 3, 3, 2, 2, 3, 2, 1, 3, 2, 0, 3, 1, 3, 2, 0, 2, 1, 3, 3, 1, 2, 2, 0, 0, 3, 0, 2, 3, 3, 0, 1, 2, 2, 3, 0, 0, 0, 2, 2, 2, 2, 2, 3, 1, 3, 3, 2, 2, 0, 2, 0, 2, 1, 2, 2, 3]\n",
            "12438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text)\n",
        "word_index = tokenizer.word_index\n",
        "X = tokenizer.texts_to_sequences(text)\n",
        "X = pad_sequences(X, padding = \"post\", truncating = \"post\", maxlen = maxLength)\n",
        "y = [to_categorical(i,num_classes = 4) for i in classification]\n",
        "#y = classification"
      ],
      "metadata": {
        "id": "liA33L5_O3ai"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed = {}\n",
        "f = open('drive/MyDrive/glove.6B.100d.txt','r')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = np.asarray(values[1:], dtype = \"float32\")\n",
        "\tembed [word] = coefs\n",
        "f.close()"
      ],
      "metadata": {
        "id": "akidn1Gy8Euv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embed.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "6Xz-XfIcPAUy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(np.array(y).shape)\n",
        "\n",
        "keywordModel = Sequential()\n",
        "keywordModel.add(Embedding(len(word_index) + 1, 64,input_length=maxLength))\n",
        "keywordModel.add(Bidirectional(LSTM(64)))\n",
        "keywordModel.add(Dense(64, activation = \"relu\"))\n",
        "keywordModel.add(Dense(4, activation = \"softmax\"))\n",
        "keywordModel.compile(loss=\"categorical_crossentropy\", optimizer = \"adam\")\n",
        "keywordModel.fit(X, np.array(y), batch_size = 32, epochs = 10, validation_split = 0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NROqJNVZ830x",
        "outputId": "f364bf59-3309-4469-bdde-df2d7ed34ee3"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 6234)\n",
            "(500, 4)\n",
            "Epoch 1/10\n",
            "15/15 [==============================] - 184s 12s/step - loss: 1.3605 - val_loss: 1.3087\n",
            "Epoch 2/10\n",
            "15/15 [==============================] - 178s 12s/step - loss: 1.3029 - val_loss: 1.2834\n",
            "Epoch 3/10\n",
            "15/15 [==============================] - 184s 12s/step - loss: 1.1662 - val_loss: 1.1062\n",
            "Epoch 4/10\n",
            "15/15 [==============================] - 181s 12s/step - loss: 0.7802 - val_loss: 0.9386\n",
            "Epoch 5/10\n",
            "15/15 [==============================] - 181s 12s/step - loss: 0.4421 - val_loss: 1.0742\n",
            "Epoch 6/10\n",
            "15/15 [==============================] - 179s 12s/step - loss: 0.1990 - val_loss: 1.1436\n",
            "Epoch 7/10\n",
            "15/15 [==============================] - 184s 12s/step - loss: 0.0929 - val_loss: 1.3865\n",
            "Epoch 8/10\n",
            "15/15 [==============================] - 180s 12s/step - loss: 0.0468 - val_loss: 1.3717\n",
            "Epoch 9/10\n",
            "15/15 [==============================] - 179s 12s/step - loss: 0.0453 - val_loss: 1.2853\n",
            "Epoch 10/10\n",
            "15/15 [==============================] - 183s 12s/step - loss: 0.0212 - val_loss: 1.3299\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f33bc352a50>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textTest = []\n",
        "classificationTest = []\n",
        "with open('/content/drive/My Drive/cs3244/balancedtest.csv','r') as f:\n",
        "  data = csv.reader(f)\n",
        "  for row in data:\n",
        "    temp.append((row[0],row[1]))\n",
        "random.shuffle(temp)\n",
        "for i in range(500):\n",
        "  textTest.append(temp[i][1])\n",
        "  classificationTest.append(int(temp[i][0])-1)\n",
        "XTest = tokenizer.texts_to_sequences(textTest)\n",
        "XTest = pad_sequences(XTest, padding = \"post\", truncating = \"post\", maxlen = maxLength)\n",
        "yTest = [to_categorical(i,num_classes = 4) for i in classificationTest]"
      ],
      "metadata": {
        "id": "w13oaXPPc0AD"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(XTest.shape)\n",
        "a = keywordModel.predict(XTest)\n",
        "a = np.argmax(a, axis = -1)\n",
        "flattened_actual = (np.argmax(np.array(yTest), axis = -1)).flatten()\n",
        "flattened_output = a.flatten()\n",
        "print(classification_report(flattened_actual, flattened_output))\n",
        "print(f1_score(y_pred=flattened_actual, y_true=flattened_output,average = \"macro\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WupRu2GxllN1",
        "outputId": "75df096d-d89a-45af-d3b1-15cdc190fc64"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 12438)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.10      0.17       135\n",
            "           1       0.35      0.57      0.43        83\n",
            "           2       0.91      0.38      0.54       183\n",
            "           3       0.29      0.79      0.42        99\n",
            "\n",
            "    accuracy                           0.42       500\n",
            "   macro avg       0.60      0.46      0.39       500\n",
            "weighted avg       0.68      0.42      0.40       500\n",
            "\n",
            "0.39058559276375193\n"
          ]
        }
      ]
    }
  ]
}